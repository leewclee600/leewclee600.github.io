{"cells":[{"metadata":{"_uuid":"0b2b92fd-e9c2-4722-9605-d77cfdc70e55","_cell_guid":"ed0c37c5-a333-4522-96b8-de0c7017caad","trusted":true},"cell_type":"code","source":"# %% [markdown]\n# # Overview & Remarks\n# \n# Direct fork from here: https://www.kaggle.com/khoongweihao/efficientnets-quantile-regression-inference\n# \n# **Diff: \"Cleaned up\" code for my own understanding**\n# \n# - Just some experiments I did with efficientnets b0-b7 and blending predictions\n# - Best LB of efficientnets was around -0.6922\n# - Tried blending efficientnets b0-b7 in a single run but due to out-of-memory errors, it was not successful\n#     - you may find the code to perform the mean blend here as well\n# - EfficientNets are trained for 30 or 50 epochs with modified callbacks and training parameters\n# - More models are being experimented currently. Will update this notebook when I have better results!\n\n# %% [markdown]\n# # Acknowledgements\n# \n# - Michael Kazachok's Linear Decay (based on ResNet CNN)\n#     - Model that uses images can be found at: https://www.kaggle.com/miklgr500/linear-decay-based-on-resnet-cnn\n# - Ulrich GOUE's Osic-Multiple-Quantile-Regression-Starter\n#     - Model that uses tabular data can be found at: https://www.kaggle.com/ulrich07/osic-multiple-quantile-regression-starter\n# - Replaced Michael's model with EfficientNets B0, B2, B4\n# - I only tweaked the parameters for the models \n\n# %% [markdown]\n# # Imports\n\n# %% [code] {\"_kg_hide-output\":true}\n!pip install ../input/kerasapplications/keras-team-keras-applications-3b180cb -f ./ --no-index\n!pip install ../input/efficientnet/efficientnet-1.1.0/ -f ./ --no-index\n\n# %% [code]\nimport os\nimport datetime\nfrom functools import lru_cache\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport tensorflow as tf \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow_addons.optimizers import RectifiedAdam\nfrom tensorflow.keras import Model\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.optimizers import Nadam\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom colorama import Fore, Back, Style\n\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(42)\nROOT = \"../input/osic-pulmonary-fibrosis-progression/\"\n\n# %% [code]\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)\n\n# %% [code]\ntrain = pd.read_csv(os.path.join(ROOT, 'train.csv'))\ntest = pd.read_csv(os.path.join(ROOT, 'test.csv'))\n\n# %% [markdown]\n# # Linear Decay (based on EfficientNets)\n\n# %% [code]\ndef get_agss_vector(df):\n    \n    \"\"\"agss = age, gender, smokingstatus\"\"\"\n    \n    normalized_age = [(df.Age.values[0] - 30) / 30] \n\n    gender = [0 if df.Sex.values[0] == 'male' else 1]\n    \n    if df.SmokingStatus.values[0] == 'Never smoked':\n        smoking_status = [0, 0]\n    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n        smoking_status = [1, 1]\n    elif df.SmokingStatus.values[0] == 'Currently smokes':\n        smoking_status = [0, 1]\n    else:\n        smoking_status = [1, 0]\n\n    vector = normalized_age + gender + smoking_status\n    return np.array(vector)\n\n# %% [code]\ndef sample_best_fit_line_weeks_vs_fvc():\n    \n    patient = train.Patient.sample().iloc[0]\n    sub = train.loc[train.Patient == patient, :]\n    fvc = sub.FVC.values\n    weeks = sub.Weeks.values\n    vals = np.c_[weeks, np.ones(len(weeks))]  # column-wise stack\n    \n    # see example https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html\n    m, c = np.linalg.lstsq(vals, fvc, rcond=-1)[0]\n    \n    print(f\"Patient number: {patient}\")\n    print(\"FVC\", fvc)\n    print(\"Weeks\", weeks)\n    print(vals)\n    print(f\"gradient: {m:.2f}\\nintercept: {c:.2f}\")\n    print()\n    _ = plt.plot(weeks, fvc, 'o', label='Original data', markersize=10)\n    _ = plt.plot(weeks, m * weeks + c, 'r', label='Fitted line')\n    _ = plt.legend()\n    _ = plt.xlabel(\"Weeks\"), plt.ylabel(\"FVC\")\n    plt.show()\n\n# %% [code]\n# gradient = rate of decay in FVC values\nsample_best_fit_line_weeks_vs_fvc()\n\n# %% [code]\ngradients = {} \nagss_vectors = {} \npatients = []\n\nfor i, patient_id in enumerate(train.Patient.unique()):\n    sub = train.loc[train.Patient == patient_id, :] \n    fvc = sub.FVC.values\n    weeks = sub.Weeks.values\n    c = np.c_[weeks, np.ones(len(weeks))]\n    gradient, intercept = np.linalg.lstsq(c, fvc, rcond=-1)[0]\n    \n    gradients[patient_id] = gradient\n    agss_vectors[patient_id] = get_agss_vector(sub)\n    patients.append(patient_id)\n\n# %% [markdown]\n# ## CNN for coeff prediction\n\n# %% [code]\ndef get_img(path):\n    d = pydicom.dcmread(path)\n    return cv2.resize(d.pixel_array / 2**11, (512, 512))\n\n# %% [code]\n# sample\n_ = plt.imshow(get_img(os.path.join(ROOT, \"train\", \"ID00007637202177411956430\", \"1.dcm\")))\n\n# %% [code]\nfrom tensorflow.keras.layers import (\n    Input,\n    Activation,\n    LeakyReLU,\n    Dropout,\n    BatchNormalization,\n    Dense,\n    Conv2D, \n    AveragePooling2D,\n    GlobalAveragePooling2D,\n    Add,\n    Flatten,\n    Concatenate,\n)\nimport efficientnet.tfkeras as efn\n\ndef get_efficientnet(model, shape):\n    models_dict = {\n        'b0': efn.EfficientNetB0(input_shape=shape, weights=None, include_top=False),\n        'b1': efn.EfficientNetB1(input_shape=shape, weights=None, include_top=False),\n        'b2': efn.EfficientNetB2(input_shape=shape, weights=None, include_top=False),\n        'b3': efn.EfficientNetB3(input_shape=shape, weights=None, include_top=False),\n        'b4': efn.EfficientNetB4(input_shape=shape, weights=None, include_top=False),\n        'b5': efn.EfficientNetB5(input_shape=shape, weights=None, include_top=False),\n        'b6': efn.EfficientNetB6(input_shape=shape, weights=None, include_top=False),\n        'b7': efn.EfficientNetB7(input_shape=shape, weights=None, include_top=False),\n    }\n    return models_dict[model]\n\ndef build_model(shape=(512, 512, 1), model_class=None):\n    \n    img_inp = Input(shape=shape, name=\"image_input\")\n    base = get_efficientnet(model_class, shape)\n    x = base(img_inp)\n    img_outp = GlobalAveragePooling2D()(x)\n    \n    # AGSS = Age + Gender + SmokingStatus\n    agss_inp = Input(shape=(4,), name=\"age_gender_smokingsstatus_input\")\n    agss_outp = tf.keras.layers.GaussianNoise(0.2)(agss_inp)\n    \n    x = Concatenate()([img_outp, agss_outp]) \n    x = Dropout(0.5)(x) \n    output = Dense(1)(x)\n    \n    model = Model([img_inp, agss_inp] , output)\n    weights = [w for w in os.listdir('../input/osic-model-weights') if model_class in w]\n    assert len(weights) == 1, \"More than one model weights match the 'model_class' substring\"\n    model.load_weights('../input/osic-model-weights/' + weights[0])\n    \n    return model\n\nmodel_classes = ['b5']  # ['b0','b1','b2','b3',b4','b5','b6','b7']\nmodels = [build_model(shape=(512, 512, 1), model_class=m) for m in model_classes]\nprint('Number of models: ' + str(len(models)))\n\n# %% [code]\nmodels[0].summary()\n\n# %% [code]\ntf.keras.utils.plot_model(\n    models[0], \n    to_file='model.png',\n    show_shapes=False, \n    show_layer_names=True,\n    rankdir='TB',\n    expand_nested=False, \n    dpi=120,\n)\n\n# %% [code]\ntrain_patients, validation_patients = train_test_split(patients, shuffle=True, train_size=0.8)\n\n# %% [code]\nsns.distplot(list(gradients.values()));\n\n# %% [code]\nDFs = {\n    \"train\": train,\n    \"test\": test,\n}\n\n# %% [code]\ndef fetch_images(patient_id, root=ROOT):\n    image_files = os.listdir(os.path.join(root, f'train/{patient_id}/'))\n    images = read_images_in_middle_of_scan(image_files, patient_id)\n    return images\n\ndef read_images_in_middle_of_scan(image_files, patient_id, lower=0.15, upper=0.8):\n    images = []\n    for filename in image_files:\n        file_no, _ = os.path.splitext(filename) # cut out '.dcm' file extension\n        file_no = int(file_no)\n        is_img_slice_in_middle = lower < file_no / len(image_files) < upper\n        if is_img_slice_in_middle:\n            image_filepath = os.path.join(ROOT, f'train/{patient_id}/{filename}')\n            images.append(get_img(image_filepath))\n    return images\n\ndef create_agss_vec_mat(patient_df, num_rows):\n    agss_vector = get_agss_vector(patient_df)\n    agss_matrix = np.array([agss_vector] * num_rows)\n    return agss_vector, agss_matrix\n\ndef filter_df_with_patient_id(df, patient_id, patient_col=\"Patient\"):\n    return df.loc[df[patient_col] == patient_id, :]\n\ndef pred_fvc(x, m, c):\n    \"\"\"\n    x --> weeks from base week\n    m --> gradient i.e. rate of FVC decay (would be -ve for a patient with disease)\n    c --> base week FVC\n    \"\"\"\n    return m * x + c\n\ndef pred_confidence(base_percent, m, gap_in_weeks):\n    \"\"\"\n    Predict confidence AKA \"std deviation\". Lower val means high confidence in predicted FVC.\n    base_percent --> percentage in the base week\n    m --> gradient i.e. rate of FVC decay (would be -ve for a patient with disease)\n    gap_in_weeks --> just the gap irrespective of whether in the past or future\n    \"\"\"\n    \n    # the formula takes into account that as prediction moves away from the base week,\n    # confidence drops (value gets bigger since m is or would be for most -ve)\n    return base_percent - m * abs(gap_in_weeks)\n\ndef score(fvc_true, fvc_pred, sigma):\n    sigma_clip = np.maximum(sigma, 70) # changed from 70, trie 66.7 too\n    delta = np.abs(fvc_true - fvc_pred)\n    delta = np.minimum(delta, 1000)\n    sq2 = np.sqrt(2)\n    metric = (delta / sigma_clip) * sq2 + np.log(sigma_clip * sq2)\n    return np.mean(metric)\n\n@lru_cache(1000)\ndef make_model_pred(df_name, patient_id, model_idx):\n    global DFs\n    df = DFs[df_name]\n    patient_df = df[df.Patient == patient_id]\n    images = fetch_images(patient_id)\n    images = np.expand_dims(images, axis=-1)\n    agss_vector, agss_matrix = create_agss_vec_mat(patient_df, num_rows=images.shape[0])\n    return models[model_idx].predict([images, agss_matrix])\n\n# %% [code]\ndef calc_patient_score(df_name, patient_id, quantile, model_idx, return_extra_vals=False):\n    global DFs\n    df = DFs[df_name]\n    patient_df = df[df.Patient == patient_id]\n    assert not patient_df.empty\n    \n    # model predicts for each image + agss_vector input\n    gradients = make_model_pred(df_name, patient_id, model_idx)\n    \n    if gradients is None:\n        return  # if no valid images in range, it will be None\n    gradient = np.quantile(gradients, quantile)  # gradient @ quantile from gradients\n\n    percent_true = patient_df.Percent.values\n    fvc_true = patient_df.FVC.values\n    weeks_true = patient_df.Weeks.values\n\n    fvc_predict = pred_fvc(x=(weeks_true - weeks_true[0]), m=gradient, c=fvc_true[0])\n    confidence = pred_confidence(base_percent=percent_true[0], \n                                 m=gradient, \n                                 gap_in_weeks=(weeks_true - weeks_true[0]),\n                                )\n    patient_score = score(fvc_true, fvc_predict, confidence)\n    if not return_extra_vals:\n        return patient_score\n    else:\n        return patient_score, gradient, fvc_predict, confidence\n\n# %% [code]\nsubs = []\nstart = datetime.datetime.now()\nfor model_idx in range(len(models)):\n    quantile_means = []\n    quantiles = np.arange(0.1, 1.0, 0.05)\n    for quantile in quantiles:\n        \n        print(f\"Quantile: {quantile:.2f}\", end=\" -->  \")\n        patient_scores_per_quantile = []\n        \n        for patient_id in validation_patients:\n            if patient_id in ['ID00011637202177653955184', 'ID00052637202186188008618']:\n                continue\n            one_patient_score_per_quantile = calc_patient_score(\"train\", \n                                                                patient_id, \n                                                                quantile, model_idx,\n                                                               )\n            if one_patient_score_per_quantile is not None:\n                patient_scores_per_quantile.append(one_patient_score_per_quantile)\n\n        mean_quantile_score = np.mean(patient_scores_per_quantile)\n        print(f\"Patient scores mean for quantile {quantile:.2f}: {mean_quantile_score:.4f}\")\n        quantile_means.append(mean_quantile_score)\n\n    sub = pd.read_csv(os.path.join(ROOT,'sample_submission.csv'))\n    test = pd.read_csv(os.path.join(ROOT,'test.csv'))\n\n    ## quantile with the smallest mean -> smallest error\n    lowest_quantile_mean_idx = np.argmin(quantile_means)\n    lowest_quantile = (lowest_quantile_mean_idx + 1) / 10\n\n    gradient_test, calc_fvc_base_test, percent_test, base_weeks_test = {}, {}, {}, {}\n    \n    # this loop defines base parameters for each patient needed to calculate week-by-week prediction\n    for patient_id in test.Patient.unique():\n        _, gradient, *_ = calc_patient_score(\"test\", \n                                             patient_id, \n                                             lowest_quantile,\n                                             model_idx,\n                                             return_extra_vals=True,\n                                            )  # only gradient needed\n        patient_df = test[test.Patient == patient_id]\n        \n        # test assumption: df will have 1 row since test set\n        assert patient_df.shape[0] == 1\n        \n        gradient_test[patient_id] = gradient  # prediction value of the model\n        \n        # pred of FVC at week 0 itself. Other weeks will be predicted using this as base\n        calc_fvc_base_test[patient_id] = (patient_df.FVC.values - \n                                          gradient * patient_df.Weeks).values[0]  \n\n        percent_test[patient_id] = patient_df.Percent.values[0]\n        base_weeks_test[patient_id] = patient_df.Weeks.values[0]\n\n    # this loop predicts values (FVC and confidence) for each patient's each week\n    for k in sub.Patient_Week.values:\n        \n        patient_id, week_no = k.split('_')\n        week_no = int(week_no)\n        \n        gradient = gradient_test[patient_id]\n        base_fvc = calc_fvc_base_test[patient_id]\n        base_percent = percent_test[patient_id]\n        base_week = base_weeks_test[patient_id]\n        gap_from_base_week = base_week - week_no\n        \n        predicted_fvc = pred_fvc(week_no, m=gradient, c=base_fvc)\n        predicted_conf = pred_confidence(base_percent,\n                                         m=gradient,\n                                         gap_in_weeks=gap_from_base_week,\n                                        )\n        \n        sub.loc[sub.Patient_Week==k, 'FVC'] = predicted_fvc\n        sub.loc[sub.Patient_Week==k, 'Confidence'] = predicted_conf\n    \n    sub_ = sub[[\"Patient_Week\", \"FVC\", \"Confidence\"]].copy()\n    subs.append(sub_)\nend = datetime.datetime.now()\nprint(end - start)\n\n# %% [markdown]\n# ## Averaging Predictions\n\n# %% [code]\nN = len(subs)\nsub = subs[0].copy() # ref\nsub[\"FVC\"] = 0\nsub[\"Confidence\"] = 0\nfor i in range(N):\n    sub[\"FVC\"] += subs[0][\"FVC\"] * (1/N)\n    sub[\"Confidence\"] += subs[0][\"Confidence\"] * (1/N)\n\n# %% [code]\nsub.head()\n\n# %% [code]\nimg_sub = sub[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()\nimg_sub.to_csv(\"submission_img.csv\", index=False)","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}